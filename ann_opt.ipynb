{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import sklearn.datasets as dataset\nimport numpy as np\nimport pandas as pd\nfrom keras.utils import np_utils\nfrom sklearn.model_selection import train_test_split\nimport random\nfrom sklearn.utils import shuffle\ndata= dataset.load_iris()\nX= data.data\nY= data.target\nY=Y.reshape(150,1,1)\nX=X.reshape(150,1,4)\nX= X/np.linalg.norm(X)\nx_train,x_test,y_train,y_test=train_test_split(X,Y,train_size=0.33,random_state=42)\ny_train=np_utils.to_categorical(y_train)\ny_test=np_utils.to_categorical(y_test)\nprint(y_test[0])","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-25T03:38:14.381809Z","iopub.execute_input":"2022-06-25T03:38:14.382068Z","iopub.status.idle":"2022-06-25T03:38:14.395897Z","shell.execute_reply.started":"2022-06-25T03:38:14.382041Z","shell.execute_reply":"2022-06-25T03:38:14.395243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Gradient descent used below","metadata":{}},{"cell_type":"code","source":"class Layer:\n    def __init__(self,input_size,output_size):\n        self.input=None\n        self.output=None\n        self.weights= np.random.rand(input_size,output_size)-0.5\n        self.biases= np.random.rand(1,output_size)-0.5\n    def forward(self,input):\n        self.input=input\n        self.output= np.dot(self.input,self.weights)+self.biases\n        return self.output\n    def backward(self,o_error,alpha):\n        w_error= np.dot(self.input.T,o_error)\n        x_error= np.dot(o_error,self.weights.T)\n        self.weights=self.weights + w_error*alpha \n        self.biases=self.biases+alpha*o_error\n        return x_error","metadata":{"execution":{"iopub.status.busy":"2022-06-25T03:23:39.591313Z","iopub.execute_input":"2022-06-25T03:23:39.591600Z","iopub.status.idle":"2022-06-25T03:23:39.600988Z","shell.execute_reply.started":"2022-06-25T03:23:39.591567Z","shell.execute_reply":"2022-06-25T03:23:39.599708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Activation_class:\n    def __init__(self,a_func,act_func_derivative):\n        self.activation= a_func\n        self.activation_derivative= act_func_derivative\n    def forward(self,input):\n        self.input= input\n        self.output= self.activation(self.input)\n        return self.output\n    def backward(self,y_error,alpha):\n        return self.activation_derivative(self.input)*y_error","metadata":{"execution":{"iopub.status.busy":"2022-06-25T03:23:36.062272Z","iopub.execute_input":"2022-06-25T03:23:36.062867Z","iopub.status.idle":"2022-06-25T03:23:36.069777Z","shell.execute_reply.started":"2022-06-25T03:23:36.062816Z","shell.execute_reply":"2022-06-25T03:23:36.068960Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class NN:\n    def __init__(self):\n        self.layers=[]\n        self.error= lambda y_pred,y_true: np.mean(np.power(y_true-y_pred,2))\n        self.error_derivative= lambda y_true,y_output: 2*(y_true-y_output)/75\n    def adding_layer(self,layers):\n        self.layers.append(layers)\n    def fit_model(self,x,y,itera,alpha):\n        for i in range(itera):\n            p_e=[]\n            net_error=0\n            for j in range(49):\n                output=x[j]\n                for layers in self.layers:\n                    output= layers.forward(output)\n                error_occured= self.error(output,y[j])\n                net_error+=error_occured\n                error_derivative= self.error_derivative(y[j],output)\n                for layers in reversed(self.layers):\n                    error_derivative=layers.backward(error_derivative,alpha,)\n            net_error/=49\n            if(i%200==0):\n                print(\"iteration: \", i,\"error: \",net_error)\n        return net_error\n    def predict(self,input):\n        samples = len(input)\n        result =[]\n        for i in range(samples):\n            output = input[i]\n            for layers in self.layers:\n                output= layers.forward(output)  \n            result.append(output)\n        return result\n    def accuracy(self,input):\n        ac = 0\n        samples = len(input)\n        for i in range(samples):\n            y_pred=self.predict(x_test[i])\n            y_pred= np.argmax(y_pred)\n            if(y_pred == np.argmax(y_test[i])):\n                ac+=1\n        acc = (ac/samples)*100\n        return acc","metadata":{"execution":{"iopub.status.busy":"2022-06-25T03:55:20.446074Z","iopub.execute_input":"2022-06-25T03:55:20.446546Z","iopub.status.idle":"2022-06-25T03:55:20.459685Z","shell.execute_reply.started":"2022-06-25T03:55:20.446512Z","shell.execute_reply":"2022-06-25T03:55:20.458703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Activation_Relu(x):\n    output = np.maximum(0, x)\n    return output\ndef Activation_Relu_derivative(x):\n     return np.greater(x, 0.).astype(np.float64)\n      \ndef Activation_softmax_derivative(x):\n    return x*(1-x)\ndef Activation_softmax(input):\n    exp_values = np.exp(input.astype(float))\n    prob= exp_values/np.sum(exp_values,keepdims=True)\n    return prob\ndef sigmoid(input):\n    return 1/(1+(np.exp(-input)))\ndef sigmoid_derivative(input):\n    return sigmoid(input)*(1 - sigmoid(input))\n ","metadata":{"execution":{"iopub.status.busy":"2022-06-25T03:24:04.730644Z","iopub.execute_input":"2022-06-25T03:24:04.730909Z","iopub.status.idle":"2022-06-25T03:24:04.738798Z","shell.execute_reply.started":"2022-06-25T03:24:04.730880Z","shell.execute_reply":"2022-06-25T03:24:04.738106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"network= NN()\nnetwork.adding_layer(Layer(4,10))\nnetwork.adding_layer(Activation_class(Activation_Relu,Activation_Relu_derivative))\nnetwork.adding_layer(Layer(10,3))\nnetwork.adding_layer(Activation_class(sigmoid,sigmoid_derivative))\nnetwork.fit_model(x_train,y_train,30000,0.5)\np=network.predict(x_test)\nq= network.accuracy(x_test)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T04:31:46.136198Z","iopub.execute_input":"2022-06-25T04:31:46.137017Z","iopub.status.idle":"2022-06-25T04:33:51.387297Z","shell.execute_reply.started":"2022-06-25T04:31:46.136953Z","shell.execute_reply":"2022-06-25T04:33:51.386392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#accuracy\nprint(q)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T04:35:11.996510Z","iopub.execute_input":"2022-06-25T04:35:11.996806Z","iopub.status.idle":"2022-06-25T04:35:12.001347Z","shell.execute_reply.started":"2022-06-25T04:35:11.996777Z","shell.execute_reply":"2022-06-25T04:35:12.000674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The accuracy is around 97 percent after 30000 iterations**","metadata":{}},{"cell_type":"markdown","source":"# **Other optimizers used**:","metadata":{}},{"cell_type":"markdown","source":"# **Momentum Gradient Descent**\nUses and additional momemtum parameter to reach the minimum faster ","metadata":{}},{"cell_type":"code","source":"class Layer_2:\n    def __init__(self,input_size,output_size):\n        self.input=None\n        self.output=None\n        self.weights= np.random.rand(input_size,output_size)-0.5\n        self.biases= np.random.rand(1,output_size)-0.5\n        self.momemtum = 0.9\n        self.m_vector_w, self.m_vector_b = 0 , 0\n    def forward(self,input):\n        self.input=input\n        self.output= np.dot(self.input,self.weights)+self.biases\n        return self.output\n    def backward(self,o_error,alpha): \n        w_error= np.dot(self.input.T,o_error)\n        x_error= np.dot(o_error,self.weights.T)\n        self.m_vector_w = self.momemtum * self.m_vector_w + alpha * w_error\n        self.m_vector_b = self.momemtum * self.m_vector_b + alpha * o_error\n        self.weights=self.weights + self.m_vector_w\n        self.biases=self.biases + self.m_vector_b \n        return x_error","metadata":{"execution":{"iopub.status.busy":"2022-06-25T04:36:05.597101Z","iopub.execute_input":"2022-06-25T04:36:05.597643Z","iopub.status.idle":"2022-06-25T04:36:05.607594Z","shell.execute_reply.started":"2022-06-25T04:36:05.597589Z","shell.execute_reply":"2022-06-25T04:36:05.606689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"network= NN()\nnetwork.adding_layer(Layer_2(4,5))\nnetwork.adding_layer(Activation_class(Activation_Relu,Activation_Relu_derivative))\nnetwork.adding_layer(Layer_2(5,3))\nnetwork.adding_layer(Activation_class(sigmoid,sigmoid_derivative))\nnetwork.fit_model(x_train,y_train,15000,0.5)\np=network.predict(x_test)\nq= network.accuracy(y_test)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T04:48:26.839378Z","iopub.execute_input":"2022-06-25T04:48:26.839691Z","iopub.status.idle":"2022-06-25T04:49:35.398967Z","shell.execute_reply.started":"2022-06-25T04:48:26.839656Z","shell.execute_reply":"2022-06-25T04:49:35.397920Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#accuracy now\nprint(q)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T04:49:41.859285Z","iopub.execute_input":"2022-06-25T04:49:41.859571Z","iopub.status.idle":"2022-06-25T04:49:41.864329Z","shell.execute_reply.started":"2022-06-25T04:49:41.859539Z","shell.execute_reply":"2022-06-25T04:49:41.863692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**In Momemtum Gradient Descent, 97 percent accuracy on test data after just 15000 iterations, which is half of what was required in simple gradient descent**","metadata":{}},{"cell_type":"markdown","source":"# **Another implementation of momemtum optimizer**","metadata":{}},{"cell_type":"code","source":"class Layer_2_1:\n    def __init__(self,input_size,output_size):\n        self.input=None\n        self.output=None\n        self.weights= np.random.rand(input_size,output_size)-0.5\n        self.biases= np.random.rand(1,output_size)-0.5\n        self.momemtum = 0.9\n        self.m_vector_w, self.m_vector_b = 0 , 0\n    def forward(self,input):\n        self.input=input\n        self.output= np.dot(self.input,self.weights)+self.biases\n        return self.output\n    def backward(self,o_error,alpha): \n        w_error= np.dot(self.input.T,o_error)\n        x_error= np.dot(o_error,self.weights.T)\n        self.m_vector_w = self.momemtum * self.m_vector_w + (1-self.momemtum) * w_error\n        self.m_vector_b = self.momemtum * self.m_vector_b + (1-self.momemtum) * o_error\n        self.weights=self.weights + alpha*self.m_vector_w\n        self.biases=self.biases + alpha*self.m_vector_b \n        return x_error","metadata":{"execution":{"iopub.status.busy":"2022-06-25T04:51:16.585533Z","iopub.execute_input":"2022-06-25T04:51:16.585858Z","iopub.status.idle":"2022-06-25T04:51:16.595764Z","shell.execute_reply.started":"2022-06-25T04:51:16.585820Z","shell.execute_reply":"2022-06-25T04:51:16.595068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"network= NN()\nnetwork.adding_layer(Layer_2_1(4,5))\nnetwork.adding_layer(Activation_class(Activation_Relu,Activation_Relu_derivative))\nnetwork.adding_layer(Layer_2_1(5,3))\nnetwork.adding_layer(Activation_class(sigmoid,sigmoid_derivative))\nnetwork.fit_model(x_train,y_train,15000,0.5)\np=network.predict(x_test)\nq= network.accuracy(y_test)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T04:55:30.798162Z","iopub.execute_input":"2022-06-25T04:55:30.798716Z","iopub.status.idle":"2022-06-25T04:56:45.844794Z","shell.execute_reply.started":"2022-06-25T04:55:30.798662Z","shell.execute_reply":"2022-06-25T04:56:45.843877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Accuracy after 15000 iterations: \", q)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T04:57:22.622134Z","iopub.execute_input":"2022-06-25T04:57:22.622441Z","iopub.status.idle":"2022-06-25T04:57:22.627778Z","shell.execute_reply.started":"2022-06-25T04:57:22.622403Z","shell.execute_reply":"2022-06-25T04:57:22.627100Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**The accuracy on test data is 97 percent after 15000 iterations**","metadata":{}},{"cell_type":"markdown","source":"# **RMS Propagation:** uses different learning rate for differnet parameters.\n Divides learning rate by the average of exponential decay of squared gradients.","metadata":{}},{"cell_type":"code","source":"class Layer_3:\n    def __init__(self,input_size,output_size):\n        self.input=None\n        self.output=None\n        self.weights= np.random.rand(input_size,output_size)-0.5\n        self.biases= np.random.rand(1,output_size)-0.5\n        self.momemtum = 0.9\n        self.rms_prop_w, self.rms_prop_b = 0 , 0\n    def forward(self,input):\n        self.input=input\n        self.output= np.dot(self.input,self.weights)+self.biases\n        return self.output\n    def backward(self,o_error,alpha): \n        w_error= np.dot(self.input.T,o_error)\n        x_error= np.dot(o_error,self.weights.T)\n        self.rms_prop_w = self.momemtum * self.rms_prop_w + (1-self.momemtum) * (w_error**2)\n        self.rms_prop_b = self.momemtum * self.rms_prop_b + (1-self.momemtum) * (o_error**2)\n        self.weights=self.weights + alpha*(w_error/(self.rms_prop_w**0.5-0.000000001))\n        self.biases=self.biases + alpha*(o_error/(self.rms_prop_b**0.5-0.00000001)) \n        return x_error   ","metadata":{"execution":{"iopub.status.busy":"2022-06-25T04:57:29.684183Z","iopub.execute_input":"2022-06-25T04:57:29.685053Z","iopub.status.idle":"2022-06-25T04:57:29.694394Z","shell.execute_reply.started":"2022-06-25T04:57:29.685015Z","shell.execute_reply":"2022-06-25T04:57:29.693706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"network= NN()\nnetwork.adding_layer(Layer_3(4,5))\nnetwork.adding_layer(Activation_class(Activation_Relu,Activation_Relu_derivative))\nnetwork.adding_layer(Layer_3(5,3))\nnetwork.adding_layer(Activation_class(sigmoid,sigmoid_derivative))\nnetwork.fit_model(x_train,y_train,5500,0.01)\np=network.predict(x_test)\nq= network.accuracy(y_test)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T05:04:17.996391Z","iopub.execute_input":"2022-06-25T05:04:17.996720Z","iopub.status.idle":"2022-06-25T05:04:50.237266Z","shell.execute_reply.started":"2022-06-25T05:04:17.996682Z","shell.execute_reply":"2022-06-25T05:04:50.236335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# accuracy\nprint(q)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T05:05:08.921531Z","iopub.execute_input":"2022-06-25T05:05:08.921819Z","iopub.status.idle":"2022-06-25T05:05:08.927617Z","shell.execute_reply.started":"2022-06-25T05:05:08.921790Z","shell.execute_reply":"2022-06-25T05:05:08.926771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Test data accuracy: 97 percent after just 5500 iterations**\n\n ","metadata":{}}]}